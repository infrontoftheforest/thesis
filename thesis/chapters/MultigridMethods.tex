% !TEX root = ../main.tex

\chapter{Multigrid Methods}
\label{chapter:MultigridMethods}

\section{Smoothers}

% TODO: source Saad 2003

% TODO: define the linear problem Ax = b

% Quick comparison to direct
% TODO: fix the LU- QR- dash formatting if necessary. ALSO THIS SHOULD BE BEFORE THE SECTION ABOUT GMRES. (or maybe introduce multigrid first, then as preconditioner later?)
For the solution of a sparse linear systems, two distinct types of methods, iterative and direct methods, can be applied. For general purposes, direct methods usually utilize various decomposition or factorization techniques such as LU, QR, Cholesky in order to achieve a solution to machine precision in a known finite amount of steps. The number of steps necessary for methods such as LU decomposition is $O(n^3)$, which increasingly disadvantageous for ever larger linear systems that are arising form the applicaiton of finite element methods.

% Advantages of relaxations: easy to implement, more general linear systems mgtut[23,24,26]
% (cf. Axelsson [6], Hackbusch [86], Varga [200])
Several variants of iterative methods exist, but for introducing and demonstrating the advantages multigrid methods, the focus will be on relaxation methods, such as Jacobi or Gauss-Seidel. These methods involve decomposion the operator $A = D - E - F$ where $D$ is the diagonal of $A$, $-E$ is the strictly lower triangular part, and $-F$ is the strictly upper triangular part. The goal is then to use this decomposition to improve the starting guess $\mathbf{x}_{0}$, of the vector of unknowns $\mathbf{x}$ iteratively.

In the Jacobi scheme, rearranging after the decomposition of $A$ gives the iteration scheme

\begin{equation}
	\mathbf{x}^{(k+1)} = D^{-1}(L + U)\mathbf{x}^{(k)} + D^{-1}b
\end{equation}

where matrix $R_J = D^{-1}(L + U)$ is defined as the Jacobi iteration matrix.

Component-wise, the updated $i$-th vector component can be obtained from the previous iteration's approximation of all other unknowns as:

\begin{equation}
	x_{i}^{(k+1)} = \frac{1}{a_{ii}}\left(b_i- \sum_{j \neq i}{a_{ij}x_{j}^{(k)}} \right).
\end{equation}

If the Jacobi method is initally used to compute an itermediate approximation, this can be weighted with a factor $\omega$ against the previous approximation, given in matrix form as

\begin{equation}
	\mathbf{x}^{(k+1)} = \left[ (1-\omega) I + \omega R_J \right] \mathbf{x}^{(k)} + \omega D^{-1}\mathbf{b}.
\end{equation}

Different values of $\omega$ determines a factor known as the relaxtion rate which will be important to the idea of multigrid methods.

Similarly to the Jacobi method, the Gauss-Seidel method can be defined in the same manner, but with all already-updated individual vector components $1, 2, \ldots, j - 1$  being used in the approximation of the $j$-th component. This can be represented in matrix form as

\begin{equation}
	\mathbf{x}^{(k+1)} = (D-L)^{-1}U\mathbf{x}^{(k)} + (D-L)^{-1}\mathbf{b}.
\end{equation}

With $R_G = (D-L)^{-1}U$ defined as the Gauss-Seidel iteration matrix, the weighted version of the same method, known as Successive Over-Relaxation (SOR) is given as

\begin{equation}
	\mathbf{x}^{(k+1)} = (D - \omega L)^{-1} \left[(1-\omega)D + \omega U \right]\mathbf{x}^{(k)} + \omega (D - \omega L)^{-1} \mathbf{b}.
\end{equation}

% residual equation

% general form
The general form of these methods which rely on splitting of $A = M - N$ can be written as
\begin{equation}
	\mathbf{x}^{(k+1)} = M^{-1}N\mathbf{x}^{(k)} + M^{-1}\mathbf{b}
\end{equation}

and convergence of the iteration to the exact solution $\mathbf{x}^*$ can be obtained by examining the iteration in the form of:

\begin{equation}
	\mathbf{x^{(k+1)}} - \mathbf{x}^* = M^{-1}N\left(\mathbf{x}^{(k)} - \mathbf{x}^* \right) = \ldots = (M^{-1}N)^{k+1}\left(\mathbf{x}^{(0)} - \mathbf{x}^* \right).
\end{equation}

The sequence can be shown to converge if and only if $\rho\left( M^{-1}N \right)) < 1$. Without computing the spectral radius, which can be expensive, any matrix norm $|| M^{-1}N || < 1$ can be used as a sufficient convergence condition. For the Jacobi and Gauss-Seidel methods, it is also sufficient that $A$ is strictly or irreducibly diagonally dominant to converge for any $\mathbf{x}^{(0)}$, as is often the case with matrices obtained from the application of finite element methods. Additionally a strictly diagonally dominant matrix which is symmetric with positive diagonal entries is positive definite, and it is possible to show that for such SPD matrices Gauss-Seidel and SOR with $\omega \in (0, 2)$ will converge. %TODO cite Saad

% Disadvantages - Read Ch2

% Outline:
% + Direct vs Iterative
% 	+ operations
%  	- LU factorization fill-in
% + Different forms of Richardson, Jacobi, SOR.
% residual, error relationship
% general form of iteration/relaxation
% + damping
% + GS
% + convergence
% smooth, fourier modes, wavenumbers (for certain matrices)
% spectral radius/convergence rate of different error modes, smoothing property
% 

Error modes

Smoothing Property

\section{Geometric Multigrid}

% change to more oscillitory on a coarser grid
% correction scheme, relax on residual equation on a coarser grid
% 

\subsection{Prolongation and Restriction}

\section{Algebraic Multigrid}

% Talk a bit more and mention primary sources as a general citation

On an unstructured grid, the geometric approach cannot be applied as the formulation is dependent on the locations of the nodes of the grid. For a grid where the locations of the grid nodes are unknown, where they are unstructured, or where an arbitrary system of equations does not refer to a physical grid, an algebraic multrigrid (AMG) method can be used instead. Instead of relying on information about the geometry of the discretization, only information from the matrix $A$ is used to determine the prolongation and restriction operators.

%TODO More on unstructured grid? Adjacency graphs

\subsection{Algebraically Smooth Error}

For algebraic multigrid, the general idea starts the same with choosing a subset of unknowns for coarsening. Similar to selecting a coarse grid in multigrid, the smooth error modes should be transfered to a coarse grid which accurately represents them, but ~~~this coarse grid must be defined algebraically.~~~ % TODO: IDK where i'm going with this -- reword it...

In the geometric case, the dominant error components after relaxation have low frequencies and are considered to be smooth. This concept of smoothness can be generalized to the algebraic case by considering the algebraically smooth error to be any error which is slowly reduced by the relaxation method, which then must be corrected on the coarser levels, i.e. an error for which $ \|(I - Q^{-1}A)e^{k}\|_A \approx \| e^{k+1}\|_A $.

For relaxation schemes such as Jacobi, Gauss-Seidel, as well as their weighted variants, the smooth error is characterized by $ Ae \approx 0 $. Additionally, error components considered to be "smooth" can be more precisely defined as having a small residual comparable to the error. These smooth errors are the eigenvectors with the smallest eigenvalues and are in the near null space of $A$. These modes are damped the least by the relaxation scheme, so the coarse level correction should be effective for this error.
%TODO cite cise-2006 amg - "near null space important" pg 4 as well as ruge1987

It is shown in StÃ¼ben for algebraically smooth error of symmetric M-matrices, the following holds:

\begin{equation}
	\sum_{i \neq j}{\frac{|a_{ij}|}{a_{ii}}\frac{(e_i - e_j)^2}{e_i^2}} \ll 1, 1 \leq i \leq n.
	\label{eq:strong_connection}
\end{equation}

This can be interpreted as such: if the first term is large, implying that the off-diagonal element is comparable to the diagonal element, then $e_i - e_j$ must be small, or in other words, the smooth error must vary slowly from $e_i$ to $e_j$. For two unknowns, a relatively large coefficient $a_{ij}$ means that $u_j$ has an important influence in determining $u_i$, meaning that the point $u_i$ has a strong dependence on $u_j$. Conversely $u_j$ is said to have a strong influence on $u_i$. 

% TODO reword. i.e. full sentences
Strength of Connection: Give a threshold $0 < \theta \leq 1$, we say that the variable $u_i$ \emph{strongly depends} on variable $u_j$ if

\begin{equation}
-a_{ij} \geq \theta \max_{k \neq i}{\{-a_{ik}\}}.
\label{eq:strong_connection2}
\end{equation}

% re-word: it's directly from cise_2006

\subsection{Interpolation Operator} %TODO: rename

For the coarse grid, a subset of points need to be chosen such that the grid is coarsened in the direction on strong connections. Strong connections indicate slowly varying smooth error, as above, which make such strongly connected points good candiates to accurately interpolate from a coarse to fine grid. This idea will be used to construct an interpolation operator $I_{2h}^h$ for a coarse grid, the selection of which will be explained in the following section. The indices can be partitioned into two sets, $C$ and $F$. The set $C$ is defined as the points corresponding coarse-grid points, also present on the fine grid. The set $F$ is correspoints to points that are only present on the fine grid. To interpolate error $e_i, i \in C$ to the fine grid $C \cup F$, the points not present in C are a weighted average:

% TODO definition of "neighborhood" of i

\begin{equation}
	(I_{2h}^h\mathbb{e})_i = 
		\begin{cases}
			e_i & \text{if $i \in C$,} \\
			\displaystyle \sum_{j \in C_i}{w_{ij}e_j} & \text{if $i \in F$,}
		\end{cases}
	\label{eq:interpolation_operator}
\end{equation}

For determining the weights $w_{ij}$, the smooth error is characterized by its small residual $r = Ae \approx 0$, with one component of the residual being $r_i = \sum_{j}{a_{ij}e_j} = 0$. The only points to be concerned with are points in the \emph{neighborhood of i} $N_i$, which are the points that even have connection to point $i$, i.e. $a_{ij} \neq 0$. Defining $S_i$ as the set of all strong connections to point $i$, the residual can be reformulated by considering three types of neighboring points to $i$:
\begin{enumerate}
	\item The set $C_i$ of C-points which strongly influence $i$, $C_i = C \cap S_i$, also called the \emph{coarse interpolatory set}
	\item The set $D_i^s$ of F-points which strongly influence $i$, with $D_i = N_i - C_i$, and $D_i^s = D_i \cap S_i$, called strong noninterpolatory connections
	\item The set $D_i^w$ of F-points which do not strongly influence $i$, $D_i^w = D_i - S_i$, called weak noninterpolatory connections
\end{enumerate}

The sum can then be regrouped in terms of point $i$ and the three sets $C_i$, $D_i^s$, and $D_i^w$:

\begin{equation}
	a_{ii}e_i \approx -\sum_{j \in C_i}{a_{ij}e_j} - \sum_{j \in D_i^s}{a_{ij}e_j} - \sum_{j \in D_i^w}{a_{ij}e_j}
\end{equation}

% TODO: elaborate why these approx can be made
For points $j \in D_i^w$, $e_j$ can be grouped with $e_i$ and points in $D_i^s$ can be approximated by 

\begin{equation}
	e_j \approx \frac{\displaystyle \sum_{k \in C_i}{a_{jk}e_k}}{\displaystyle \sum_{k \in C_i}{a_{jk}}}.
	\label{ej_approx}
\end{equation}

Then the weights can be represented by

\begin{equation}
	w_{ij} = - \frac{a_{ij} + \displaystyle \sum_{m \in D_i^s}{\left(\frac{a_{im}a_{mj}}{\displaystyle \sum_{k \in C_i}{a_{mk}}}\right)}}{a_{ii} + \displaystyle \sum_{n \in D_i^w}{a_{in}}}
\end{equation}

\subsection{Coarse Grid} %TODO: rename

Now that the interpolation operator has been defined, it is now necessary to determine which points will be in the coarse interpolatory set for each point on the fine grid. These points will make up the coarse grid. The approximation made in Equation~\ref{ej_approx} is more accurate the more strongly connected $j$ is to $i$, so this can be used as a heuristic criterion for choosing the coarse grid points. Another criterion is necessary to also limit the size of the coarse set. These two criteria are given by StÃ¼ben as:
% TODO: reference Ruge/StÃ¼ben's defn of heuristics

\begin{itemize}
	\item (C1) For each $i \in F$, each point $j \in S_i$ should either be in $c$, or should be strongly connected to at least one point in $C_i$.
	\item (C2) $C$ should be a maximal subset of all points with the property that no two C-points are strongly connected to each other.
\end{itemize}

% explanation on why C2 is needed: it strikes a balance on the size of the coarse grid, whihc should be a small fraction of the total points to reduce the work per cycle.
The criterion (C2) is motivated by limiting the amount of numerical work done on the coarse grid, since no two C-points being directly connected will lead to a lesser amount of coarse grid points. In some situations both of these criteria cannot be fulfilled, so (C1) is considered to be necessary, while (C2) is used as a guideline to achieve (C1).

The coarsening process which adheres to the two criteria is a two-pass algorithm. The first pass is a quick preliminary partition of the grid into $C$- and $F$-points, while ensuring (C2) as well as possible. The second part consists of computing the weights for the interpolation operator while changing any intial $F$-points to $C$-points in order to sastisfy (C1).

% in discussion about the heuristics, talks about cise_2006 page 6 about bad performance and \theta param as perhaps a way to lead into aggregation, or not idk

% TODO: cite Falgout CiSE_2006_amg
The entire algorithm presented is highly dependent on a proper choice of $\theta$ in Equation~\ref{eq:strong_connection2}. Falgout noted that an improper choice could lead to slower performance due to too many points being considered strong connections and resulting in too large of a coarse grid.

% maybe developments in c-AMG since ruge-stueben? More research... glaze over it and cite a bunch of papers and their contributions? Then 

% aggregation

\section{Smoothed Aggregation Multigrid}

% Eventually... Parallelization
